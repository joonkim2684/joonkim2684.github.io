<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joonkim2684.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joonkim2684.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-27T11:03:11+00:00</updated><id>https://joonkim2684.github.io/feed.xml</id><title type="html">blank</title><subtitle>Hello! </subtitle><entry><title type="html">Two Interesting Randomized Algorithms (1)</title><link href="https://joonkim2684.github.io/blog/2025/Two-Randomized-Algos-Part1/" rel="alternate" type="text/html" title="Two Interesting Randomized Algorithms (1)"/><published>2025-09-27T00:00:00+00:00</published><updated>2025-09-27T00:00:00+00:00</updated><id>https://joonkim2684.github.io/blog/2025/Two-Randomized-Algos-Part1</id><content type="html" xml:base="https://joonkim2684.github.io/blog/2025/Two-Randomized-Algos-Part1/"><![CDATA[<h3 id="cs174-and-what-it-taught-me">CS174 and What it Taught Me</h3> <p>I am writing this post to appreciate once more the absolutely fascinating contents of CS174: Combinatorics and Discrete Probability, taught by the quite legendary Professor <a href="https://people.eecs.berkeley.edu/~sinclair/">Alistair Sinclair</a>. I have to mention that the course description itself is quite misleading (and scares away most CS students) because the actual content is rooted in <em>randomized algorithms</em>, which is a class of algorithms that allows the use of a perfectly random bit string for decision making. Interestingly, just by allowing the computer to sometimes make random choices, we can obtain extremely simple and elegant solutions to seemingly hard problems (with some caveats, of course). Other times, we require the output to be randomized, which requires the algorithm to be randomized in the first place. And inevitably, probability comes in for the analysis of runtime and correctness.</p> <p>What was truly inspiring for me was the fact that we can get so far in analysis without even invoking the concept of variance or other advanced topics in probability. Simple distributions and expectation arguments can be powerful, if used correctly. The two algorithms I am about to present showcase just that, albeit they challenge you to understand randomness at a deeper level. This is part 1 of this series, covering one of the two that I prepared. I left some very simple exercises for the reader to fill in the details because that’s what math textbooks do. Just kidding, to make the reading a bit more interactive and for sanity checks.</p> <h3 id="simulating-a-biased-coin-with-an-unbiased-coin">Simulating a Biased Coin with an Unbiased Coin</h3> <p>Given an unbiased (fair) coin with heads probability \(1/2\), how can you simulate any biased coin with head probability \(p\)?</p> <h4 id="a-naive-first-try">A Naive First Try</h4> <p>Let’s do a quick example with \(p=1/4\), that is, we want to return heads only 1 out of 4 times. It is not difficult to observe that we only have to flip twice and report heads for the sequence <strong>HH</strong> and tails for all others. Another example, \(p=5/8\). This is a bit more complicated, but in fact we only need three flips [Exercise: why? Hint: Consider predefining what to be heads]. The trend goes on, and it seems that we can simulate any \(p=n/2^m\) with only \(m\) flips! Is this it? Did we just find a good algorithm? To convince you that we’re not done yet, I’d like to take a small digression to a different question: the converse.</p> <h4 id="digression-the-converse">Digression: The Converse</h4> <p>Perhaps you are familiar with the converse question: how to simulate an unbiased coin with a biased one with head probability \(p\)? The answer is quite simple: flip the coin twice repeatedly until you get differing sides and report the first one. The analysis is that the sequence <strong>HT</strong> and <strong>TH</strong> must have the same probability \(p(1-p)\) of showing up, so we can take the first one to be <strong>H</strong> and the second <strong>T</strong> WLOG.</p> <p>What this answer lacks is the algorithmic analysis: it’s surely correct, but how <em>efficient</em> is it - in other words, how <em>fast</em> is it? Consider the case of a heavily biased coin, say \(p = 0.99\). This is quite bad, since our probability of hitting either stopping condition is extremely low: \(2*0.99*0.01 = 0.0198\). Then, by the expectation of a geometric, we would need to run \(1/0.0198 \approx 50.5\) iterations of two flips, so 101 flips in expectation. In fact, this algorithm is guaranteed to perform worse as \(p\) gets very large or very small [Exercise: why both?]. So in some sense, it counteracts its purpose of correcting (potentially very heavily) biased coins by design by taking longer the more biased the coin is!</p> <h4 id="the-real-problem">The “Real” Problem</h4> <p>The moral of the story for the converse problem is that we probably do not want an algorithm that performs poorly for some edge cases. Turning back to our original proposal, we now face a new challenge: can we say that our algorithm truly runs efficiently for all possible \(p\) values? I’ll give you a good one - \(p = 5/2^{10000000000}\). This is a valid \(p\) since it is between [0, 1]. However, we need 10e10 flips! Even worse: \(p=e/3\). Here, we can’t even define its denominator as a power of 2. More generally, since our algorithm needs to know in advance exactly what the \(log_2\) of the denominator will be, it fails miserably when the denominator is simply not a power of two.</p> <p>If you know some concepts in countability or real analysis, this observation should immediately trigger a fire alarm in your brain. Indeed, our algorithm succeeds for countably many values of \(p\), but it fails for <em>uncountably</em> many of them as well. Putting it another way: for all possible values of \(p\), the successful values are infinitely smaller than the unsuccessful ones. This is extremely concerning, since we want our algorithm to be generalizable to all values of \(p\), but we realized that we have some mines in our backyard, and in fact, they practically cover the entirety of the backyard! And even without fancy knowledge, you should have figured out by now that our algorithm needs some refinement.</p> <h4 id="being-lazy-is-being-smart">Being Lazy is Being Smart</h4> <p>It seems that the problem is that we are trying for infinite precision. Really, do we have to flip 10e10 flips just to say that it’s tails the vast majority of the time? Can we do something a bit smarter?</p> <p>Turns out this is the key intuition for solving the mess I described above. First, we need to change our perspective. We know that every number, even real numbers, can be represented as binary numbers [Exercise: Construct an explicit converter between decimal and binary numbers, or just convince yourself of this claim]. And since \(p \in [0, 1]\), it will always be in the form of \(p = 0.b_1b_2b_3...\) where each \(b_i\) is a binary digit [Exercise: Why?]. What this expression reveals is that even though the binary strings may be extremely long or even infinite, we only need to look at the first few. Think of it this way: we know that flipping \(n\) times gives us a uniform sampler for \(2^n\) elements. If we treat the flips as binary strings as well, as \(0.f_1f_2f_3...\) where \(f_i\) is 0 if the i-th flip is heads and 1 if tails. Then, if we have \(b_1=0\) and \(f_1\), we already know that we are “unluckier than” the threshold needed to push us into returning tails, since \(p&lt;1/2\). Turning back to predefining a “Heads” set, if we line up all possible strings in lexicographical order, and take the first \(p\) portion of them to be in the “Heads” set, we already know that even if we sample infinitely many flips from now on, we are assured to never land on the “Heads” set . However, if the two are the same, then we can conclude nothing. So we can simply do the same procedure until the digits differ [Exercise: Apply the same logic to \(b_0=1\) and convince yourself that this works].</p> <h4 id="the-algorithm-and-analysis">The Algorithm and Analysis</h4> <p>Here, I give the formal algorithm in pseudocode.</p> <p>Input: \(p\)</p> <p>Output: H/T according to a simulated biased flip of heads probability \(p\)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>do

    b &lt;= 1 if {(p * 2) &gt;= 1} else 0
    p &lt;= decimal(p * 2)

    f &lt;= flip_fair_coin()

    if {b != f}
        out &lt;= T if {f == 1} else H
        return out

while
</code></pre></div></div> <p>In this implementation, b extracts the binary digit that immediately follows the decimal point, and p is updated such that the next most significant digit is elevated to its place. Then, a random fair coin flip is compared with it to terminate the algorithm upon differing. The correctness of the algorithm was somewhat justified above, and a more advanced, intuitive argument is given in the following section. Feel free to skip it, but at least there should be some hunch that this actually works as intended by now [Challenge Exercise: Rigorously prove the correctness].</p> <p>The truly surprising part is the analysis of its runtime, and it is quite terse. The expected runtime of this algorithm is the time it takes for a bit to differ from some other predefined bit b. This is simply the expectation of a geometric of parameter 1/2, which is 2! What this means is that running a bunch of these biased coin simulations would only incur double the amout of flips that is needed to sample the same number of fair coins. In the asymptotics language, this reduces down to \(O(2) = O(1)\), which is just saying that the runtime of the algorithm is completely independent of the input. This was what we wanted! The algorithm works for any real number that is a valid probability, since nowhere in the pseudocode does it require \(p\) to be anything more specific.</p> <h4 id="a-posteriori-intuition-binary-search-advanced">A Posteriori Intuition: Binary Search [Advanced!]</h4> <p>Sampling a biased coin is the same as selecting a random point on a continuous interval [0, 1] and testing if that number is less than \(p\). So we can make an analogy to binary search, where you move down the decision tree for every new flip. The tree that we are concerned with has infinite depth and infinitely many “leaf nodes” that biject with each real number on the real number line. (The term “leaf node” is a bit handwavy since the tree is infinite, but for the sake of the intuition, we can imagine that they “exist.”) The value \(p\) is also associated with a “leaf node,” and there is a unique path from the root to it [Exercise: Why unique? Hint: Property of trees]. Then, a heads returned from a biased coin flip can be thought of as landing somewhere left of the \(p\) node. Since every flip has equal probability at every stage, it will eventually sample a uniformly random “leaf node.” Now, at any point of the traversal, if the path diverges from \(p\)’s path, we can ascertain that we fall into either the left or right side of \(p\). So we are done. Return H/T according to your findings.</p> <h4 id="conclusion">Conclusion</h4> <p>If you think about it, this is probably how real computers give you random numbers, since computers can easily sample “fair” flips. The upshot of this is that we are now free to use any biased coin flips in the analysis of other randomized algorithms without worrying about the runtime of that sampling, which can simplify things a lot! Finally, the thought process behind the algorithm illustrates how an alternative representation of the same quantity can open up new ideas for design.</p>]]></content><author><name></name></author><category term="algorithms"/><category term="algorithms,"/><category term="probability"/><summary type="html"><![CDATA[I present two very interesting algorithms I learned in CS174, based only on elementary probability theory. Part 1.]]></summary></entry><entry><title type="html">Advent of Code 2024 Week 2 Reflection</title><link href="https://joonkim2684.github.io/blog/2024/AoC2024-Week2/" rel="alternate" type="text/html" title="Advent of Code 2024 Week 2 Reflection"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://joonkim2684.github.io/blog/2024/AoC2024-Week2</id><content type="html" xml:base="https://joonkim2684.github.io/blog/2024/AoC2024-Week2/"><![CDATA[<p>A day late, but still going strong. I really shouldn’t be doing this considering I have finals in two days, but I excuse myself for developing “problem solving skills.” And in fact, some of the concepts in scope for my finals popped up in some of the problems, so maybe it wasn’t all for nothing. I started taking notes after each day instead of trying to recall all questions at once, so expect each section to be much more concise.</p> <h3 id="day-8-resonant-collinearity">Day 8: Resonant Collinearity</h3> <p>Reused some components from Day 6, adding some helper functions to increase productivity (by a lot). I also made use of some imports, which were also very helpful in turning high-level ideas into code. The implementation was straightforward using some vector manipulation techniques. This probably when I started noticing that <em>sets</em> are pretty useful data structures…</p> <ul> <li>Time: ~ 40 min</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-9-disk-fragmenter">Day 9: Disk Fragmenter</h3> <p>Basically brute forced both parts, but I had to employ a fundamentally different data structure for Parts 1 and 2. Not the most ideal situation, but it was hard to expect what was coming out in Part 2, so I greedily optimized for Part 1 only. And then I misunderstood Part 2, costing me at least 30 minutes. In retrospect, I feel like finding the index of each file could have been much more efficient with the appropriate use of heaps or stacks, but I was too lazy to do some clever tricks at this point. The final solution runs in a reasonable amount of time, and for me, that’s that.</p> <ul> <li>Time: ~ 1 hr 50 min</li> <li>Difficulty: 3.5/5</li> </ul> <h3 id="day-10-hoof-it">Day 10: Hoof It</h3> <p>Somehow misunderstood Part 1 and solved Part 2??? Turns out I wasn’t the only one, according to reddit. As soon as I read the description, my mind was all into DP, which turned out to be what Part 2 was asking for. Part 1 was more of a typical DFS search problem. Unexpectedly, both parts were correct on the first try, which saved me a bunch of debugging time.</p> <ul> <li>Time: ~ 35 min</li> <li>Difficulty: 2.5/5</li> </ul> <h3 id="day-11-plutonian-pebbles">Day 11: Plutonian Pebbles</h3> <p>Beware of the short Part 2… It was alright for me, though. Part 1 was straightforward BFS, and Part 2 was an exponentially bigger problem, unable to brute-force with a naive search tree expansion. I used a quick and dirty memoization table to cache all previous results. Of course, this is not the optimal way to cache results, since I could have memoized all subproblems (stones after 1 blinking, 2 blinkings, etc…) for a given instance, but turns out my “suboptimal” strategy still gives a perfectly fast enough runtime. In fact, it was able to give 125 blinks a quick answer! The only quirk I had with this problem was after solving it on my own, realizing there was a @cache decorator in python which basically achieves what I implemented by hand, probably much better.</p> <ul> <li>Time: ~ 30 min</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-12-garden-groups">Day 12: Garden Groups</h3> <p><strong>THE</strong> hardest question so far, by a large margin. Part 1 was a pretty solid example of a DFS “flood fill” type problem. The issue came in Part 2, where the implementation of finding the sides of a polygon involved a lot of edge case handling, which meant that debugging was more or less a given. In the end, my code turned out to be a rambling of functions to somehow patchwork and make my existing (broken) code work properly. For debugging, I had to backtrack a small 2D board by hand to figure out the very small detail I was missing. Unfortunately, only <em>one out of five</em> examples provided in the website actually triggered the bug, so I was confused for a good while. Still, tiring but satisfying to break the challenge by myself.</p> <ul> <li>Time: ~ 2 hr</li> <li>Difficulty: 4/5 (nasty bugs!)</li> </ul> <h3 id="day-13-claw-contraption">Day 13: Claw Contraption</h3> <p>As soon as I saw the problem description, I knew I had to solve this problem with (I)LP. And I did. Just to visit reddit and find out that the 2x2 matrix could have simply been solved using the “Cramer’s Rule.” Oh well, but I would like to regard this as a good practice for my upcoming CS170 finals, which covers LP anyways. For Part 2, the package I used was somehow not robust enough (?) to find all ILP solutions, and there was no way I was going to fight with this random python package that really has nothing to do with my problem solving skills. So I just found a reddit solution what checks integrality by rounding and observing its difference with the original. Don’t regret it, wise use of my time.</p> <ul> <li>Time: ~ 1 hr (time mostly spent on parsing &amp; studying the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.milp.html">scipy.optimize.milp package</a>)</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-14-restroom-redoubt">Day 14: Restroom Redoubt</h3> <p>Extremely straightforward Part 1, and then perhaps the most <em>un-straightforward</em> Part 2? The easter egg reveal in Part 2 really fely like one, and after trying out some ideas for a while, I looked up reddit for some insights. Turns out the answer was quite a simple heuristic that I might have been able to guess if given enough time. However, I had a nasty off-by-one error in my output, which cost me some time. Seems like this got some undeserved hate in reddit, but I truly appreciate the problem designer for this one.</p> <ul> <li>Time: ~ 50 min</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-15-warehouse-woes">Day 15: Warehouse Woes</h3> <p>Vector operations from Day 8 coming back clutch and saves the day! This was probably the most amount of code I had to write for a single problem so far. The logic itself wasn’t too hard to grasp in principle, but implementing pushing boxes up and down in Part 2 involved some convoluted recursive condition checking. At first, I tried being sneaky and attempted to check for conditions <em>and</em> move the boxes at the same time, but I quickly realized that the effort wasn’t worth it and resorted to splitting up into two parts, one pure and the other nonpure. Fortunately, I had no critical bugs on my first implementation round such that I was able to reproduce the sample input fairly quickly. I went for modularizing each condition-checking phase and did some leap of faith that each function works as intended, which boosted my productivity a lot. Definitely needed some structure for a problem this involved, and it paid off nicely.</p> <ul> <li>Time: ~ 1 hr 40 min</li> <li>Difficulty: 3.5/5</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Still learning a lot by solving problems. While these may not be challenging as Leetcode Hard (or even Medium) problems, the spirit of “anything’s allowed” really motivates me to think outside the box when devising approaches. The most absurd solution I saw was somebody on reddit compressing each board state into a .png file for Day 14 and observing ones with the smallest file sizes! Also, intended or not, this week’s batch of problems let me implement ideas I learned in CS170(Algorithms) in a concrete problem setting, which was very rewarding.</p> <p>I’m probably taking some time off of Advent of Code until my finals are over on the 20th, which means the next post will most likely cover all of the remaining days (Day 16-25).</p>]]></content><author><name></name></author><category term="coding"/><category term="AoC-2024"/><summary type="html"><![CDATA[Reflection on participating on AoC2024 Day 8-15.]]></summary></entry><entry><title type="html">Advent of Code 2024 Week 1 Reflection</title><link href="https://joonkim2684.github.io/blog/2024/AoC2024-Week1/" rel="alternate" type="text/html" title="Advent of Code 2024 Week 1 Reflection"/><published>2024-12-06T00:00:00+00:00</published><updated>2024-12-06T00:00:00+00:00</updated><id>https://joonkim2684.github.io/blog/2024/AoC2024-Week1</id><content type="html" xml:base="https://joonkim2684.github.io/blog/2024/AoC2024-Week1/"><![CDATA[<p>I decided to take on this year’s Advent of Code for a personal challenge. While I might miss out on a few down the road due to final exam season, I will try to be consistent as possible. My solutions can be found <a href="https://github.com/joonkim2684/AoC2024">here</a>.</p> <p>The language of choice was Python3, as I am most comfortable with it (and can sometimes do really hacky things to speed up the coding). I could have been ambitious to practice more low-level languages like C, Go, or Rust, but I concluded that my first try should be as less painstacking as possible or I might lose interest in the first place. For the sake of not spending too much time, I set my standards not on making the best classes and frameworks for each question, but rather a function-based top-down approach, where I declare all functions needed to solve the problem first, then go onto the main body to utilize the functions. Also, optimizing the code for performance wasn’t a big consideration, unless the question actually forced me to think about cutting corners in terms of runtime.</p> <p>These posts will mainly be about reflecting on my coding practices and decision-making procedures, before I forget what I actually did during the coding sessions. Not all of the comments are going to be exactly coherent, but here it goes.</p> <h3 id="day-1-historian-hysteria">Day 1: Historian Hysteria</h3> <p>Really nothing much to comment here, since there were no room for optimization nor was there an obvious need for it. Implementing the feature word-by-word was sufficient.</p> <ul> <li>Time: ~ 10 min</li> <li>Difficulty: 1/5</li> </ul> <h3 id="day-2-red-nosed-reports">Day 2: Red-Nosed Reports</h3> <p>Deceptively challenging. Part 1 was a breeze since a single pass was enough to fully determine both the increasing/decreasing status from the first two elements, and things are easy from there. However, Part 2 did not allow a naive solution. Eventually, I had to come up with a way to circumvent the fact that the first two elements could be misleading if one of them were supposed to be removed, and I am pretty happy about how I did it. It extensively exploited the fact that <em>only one</em> element could be removed, and tested from both sides. If the first two elements were the problem, they would be the last two elements on the reversed list, and the new first two elements can be tested safely. I acknowledge that my code wasn’t the most handsome-looking since there were considerable repetition for the checkIncreasing and checkDecreasing functions, but I felt that it wasn’t worth trying to abstract it beyond this point.</p> <ul> <li>Time: ~ 40 min</li> <li>Difficulty: 2.5/5</li> </ul> <h3 id="day-3-mull-it-over">Day 3: Mull It Over</h3> <p>Remember that I mentioned Python can sometimes to hacky stuff? This was one of them. The hardest part of this problem wasn’t algorithm implementation but me struggling with regex, trying to get it work with all of the escape sequences. Once I cleanly parsed all of the relevant strings, I simply called eval() on each of the multiplications, and that was that. Part 2 was barely an overhead, too.</p> <ul> <li>Time: ~ 25 min (Fighting with regex probably took like 15)</li> <li>Difficulty: 1.5/5</li> </ul> <h3 id="day-4-ceres-search">Day 4: Ceres Search</h3> <p>This question folded up nicely because the structure I built on Part 1 was also perfect for Part 2. Maybe lucky, maybe skill. There were some overhead in hard-coding the direction vectors for both parts, but it wasn’t too much of a hassle compared to the satisfying solution I ended up with. The question looked quite complicated, but my initial structure of checkIndex-&gt;checkDirection was strong enough to declutter most of the difficulty. Moral of the story: just the right amount of abstraction is the best.</p> <ul> <li>Time: ~ 25 min</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-5-print-queue">Day 5: Print Queue</h3> <p>Around this point, I was thinking about whether the data structures or algorithms I learned in school might be useful. And at first glance, this problem looked like a DAG structural problem, with the page numbers as vertices and each left-right page rule as edges. But turns out, that was way too much thinking. Part 1 was solved easier than I expected, without the use of any sophisticated CS knowledge. Then, Part 2 was when my studying shined. It wasn’t exactly an implementation of a specific algorithm, but I noticed that after traversing the entire ruleset and swapping and inconsistencies, the number of inconsistencies <em>must</em> decrease by at least one, inspired by the <strong>local search</strong> method and properties of DAG I learned in CS188(Intro to AI). Thus, the number of traversals of an inconsistent page list could be bounded by the number of total rules. This made computation much more managable, and I fearlessly looped entire traversal without worrying about runtime or infinite loops. A satisfying moment, to say the least.</p> <ul> <li>Time: ~ 20 min</li> <li>Difficulty: 2/5</li> </ul> <h3 id="day-6-guard-gallivant">Day 6: Guard Gallivant</h3> <p>Now we are getting into the deep forests more and more, where brute force methods might or might not be the best answer. Complexity of the system was obviously much greater than anything I had encountered before, since the state space was in 2-D instead of a 1-D array. This meant some clever tricks needed to be employed, if I don’t want &gt;10 minutes of just running the python script. I had to declare a lot more functions than before, since there were significantly more moving parts of the problem, such as keeping track whether the guard is out of bounds or dealing with change of direction. Nonetheless, I managed to write up a fairly compact mechanism for updating the guard in each timestep, with some ideas inspired by linear algebra and physics (hence, the variable names position and velocity). Critisisms are welcome for the messy notation of accessing the board elements, though. At second glace, I could have just made getter/setter functions for the board to mimic OOP a little bit. Part 2 was a bit of pain, as I missed out on the edge case when two or more consecutive turns are needed for the correct behavior. Was it intentional that Part 1 didn’t have any of these? Maybe. Also, I realized half-way through that the only tiles that are relevant to Part 2 were exactly the set of tiles I have colored in Part 1, which greatly improved the runtime, not exactly asymtotically but heuristically a lot.</p> <ul> <li>Time: ~ 1 hr 10 min (Debugging though…)</li> <li>Difficulty: 3/5</li> </ul> <h3 id="day-7-bridge-repair">Day 7: Bridge Repair</h3> <p>Ok, I have to admit that I made a pretty stupid mistake on my first run. Implementing the tree search recursion itself wasn’t too hard, and I managed to pass Part 1 on the first try. The real trouble was in Part 2, where I knew the “hard” recursion step wasn’t what’s broken but still not getting the correct answer. Eventually, I gave up and debugged by comparing the outputs of a correct answer from Reddit. Again, feel free to criticize for “cheating”, but I am not trying to challenge the leaderboard at any rate, and me getting to enjoy the process was more important that adhering to strict morals when all I am trying to do is a personal challenge. Ultimately, the incorrect part was a simple fix of enforcing the list to be empty when there is a match between the calculated number and the goal number, a minor oversight that cost me more than half an hour. What was truly interesting was that I managed to pass Part 1 <em>again</em> without accouting for the obviously incorrect treatment of edge cases that caused an issue in Part 2. Now I am truly beginning to suspect that the creator of the problems intended some incorrect implementations to pass Part 1 but not Part 2. Frankly, this was annoying and deeply motivating at the same time. I will definitely try not to make these mistakes going forward.</p> <ul> <li>Time: ~ 1 hr 20 min</li> <li>Difficulty: 2/5 not counting the bug, 3/5 in reality</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Even though I am a firm believer that doing is superior over seeing for studying, it was always difficult to just stand up and start tackling some coding problems. But the mere fact that I am working with other people on the same problem at the same time (even if there are no explicit interactions) enabled me to thoroughly enjoy the Advent of Code experience up until now. I expect the problems to increase in difficulty much more on the following weeks, and I can’t say for certain that I will be able to keep up with the pace, especially since Finals are coming up. Regardless, I hope I will be writing my second AoC post next week.</p>]]></content><author><name></name></author><category term="coding"/><category term="AoC-2024"/><summary type="html"><![CDATA[Reflection on participating on AoC2024 Day 1-7.]]></summary></entry><entry><title type="html">Hello!</title><link href="https://joonkim2684.github.io/blog/2024/Hello/" rel="alternate" type="text/html" title="Hello!"/><published>2024-11-25T00:00:00+00:00</published><updated>2024-11-25T00:00:00+00:00</updated><id>https://joonkim2684.github.io/blog/2024/Hello</id><content type="html" xml:base="https://joonkim2684.github.io/blog/2024/Hello/"><![CDATA[<p>Hello! I am launching my personal homepage, and this finally gave me enough excuse to start a blog channel.</p> <p>I am currently a sophomore studying computer science at UC Berkeley. My interest is on the broad spectrum of CS theory, artificial intelligence, and how those concepts actually get applied to reality.</p> <p>I envision this blog space to be mostly devoted to a mixture of paper reviews, book reviews, algorithm explanations (mostly for my reviewing’s sake), personal endeavors at learning new stuff, and maybe some life updates.</p> <p>Upload frequency would probably be extremely irregular. I will not attempt to upload a blog post just because it’s been a while. But who knows? I might get addicted later on. Or this just may as well be the first and last blog post ever.</p> <p>Lastly, thank you for visiting my barren website (for now!).</p>]]></content><author><name></name></author><category term="random"/><category term="starting-out"/><summary type="html"><![CDATA[Starting Remarks]]></summary></entry></feed>